{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vl_0Crs3HqEz"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset for training and testing\n",
        "train_data = datasets.MNIST(\n",
        "    root='data',          # Directory to store the dataset\n",
        "    train=True,           # Load the training set\n",
        "    transform=ToTensor(), # Apply ToTensor image transformation\n",
        "    download=True         # Download the dataset if not already present\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,          # Load the test set\n",
        "    transform=ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Check the size of the datasets (number of samples, width, height)\n",
        "print(train_data.data.size())  # Output: torch.Size([60000, 28, 28])\n",
        "print(test_data.data.size())   # Output: torch.Size([10000, 28, 28])\n",
        "\n",
        "# Check the labels for the training data\n",
        "print(train_data.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR-Z9eOnHvqr",
        "outputId": "bb2d931d-b80b-476f-c701-aeeaee59a2cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 13167454.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 394998.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3437466.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9937678.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([10000, 28, 28])\n",
            "tensor([5, 0, 4,  ..., 5, 6, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training and test sets\n",
        "loader = {\n",
        "    'train': DataLoader(train_data,\n",
        "                        batch_size=125,  # Number of samples per batch\n",
        "                        shuffle=True,    # Shuffle the data at every epoch\n",
        "                        num_workers=1),  # Use 1 worker for data loading\n",
        "    'test': DataLoader(test_data,\n",
        "                       batch_size=125,   # Number of samples per batch\n",
        "                       shuffle=False,    # No need to shuffle test data\n",
        "                       num_workers=1)    # Use 1 worker for data loading\n",
        "}"
      ],
      "metadata": {
        "id": "deLSm-hRIBJT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Convolutional NN model class\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # First convolutional layer: 1 input channel (grayscale), 10 output channels, kernel size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "\n",
        "        # Second convolutional layer: 10 input channels, 20 output channels, kernel size 5x5\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        # Dropout layer after the second convolution\n",
        "        self.conv2_dropout = nn.Dropout2d()\n",
        "\n",
        "        # Fully connected layer: 320 input features, 50 output features\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "\n",
        "        # Fully connected output layer: 50 input features, 10 output features (10 digits in MNIST)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first convolution, followed by ReLU and max pooling\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        # Apply second convolution, followed by dropout, ReLU, and max pooling\n",
        "        x = F.relu(F.max_pool2d(self.conv2_dropout(self.conv2(x)), 2))\n",
        "\n",
        "        # Flatten the tensor into a 1D vector (for input to fully connected layer)\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Apply first fully connected layer followed by ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply dropout (during training only)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Apply second fully connected layer (output layer)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Use log_softmax as the final activation since we're using CrossEntropyLoss\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "kj0qwMT1IBkj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model and move it to the appropriate device (CPU or GPU)\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define the optimizer (Adam) and the learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss, which combines softmax and negative log likelihood)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "EG10ZwkEINqH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the training loop\n",
        "def train(epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(loader['train']):\n",
        "        data, target = data.to(device), target.to(device)  # Move data to the correct device (CPU or GPU)\n",
        "        optimizer.zero_grad()  # Zero out gradients from previous step\n",
        "        output = model(data)   # Forward pass\n",
        "        loss = loss_fn(output, target)  # Compute the loss\n",
        "        loss.backward()  # Backpropagate the gradients\n",
        "        optimizer.step()  # Update the model's weights\n",
        "        if batch_idx % 20 == 0:  # Print progress every 20 batches\n",
        "            print(f'Train epoch: {epoch} [{batch_idx * len(data)} / {len(loader[\"train\"].dataset)}]')\n"
      ],
      "metadata": {
        "id": "wYaCwstxIOTl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the testing loop\n",
        "def test():\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for data, target in loader['test']:\n",
        "            data, target = data.to(device), target.to(device)  # Move data to device\n",
        "            output = model(data)  # Forward pass\n",
        "            test_loss += loss_fn(output, target).item()  # Accumulate test loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
        "\n",
        "    avg_loss = test_loss / len(loader['test'])  # Compute average loss per batch\n",
        "    accuracy = 100 * correct / len(loader['test'].dataset)  # Compute accuracy percentage\n",
        "    print(f'\\nTest set: Average loss: {avg_loss:.4f}, Accuracy: {correct} / {len(loader[\"test\"].dataset)}, {accuracy:.0f}%\\n')\n"
      ],
      "metadata": {
        "id": "bsjZPnIUIQgd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the training and testing loops for 10 epochs\n",
        "for epoch in range(1, 11):\n",
        "    train(epoch)  # Train the model for one epoch\n",
        "    test()        # Test the model after each epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOVWDeJ_IR6z",
        "outputId": "9d7b9e13-cb43-410d-9cb0-f056fa0ac3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch: 1 [0 / 60000]\n",
            "Train epoch: 1 [2500 / 60000]\n",
            "Train epoch: 1 [5000 / 60000]\n",
            "Train epoch: 1 [7500 / 60000]\n",
            "Train epoch: 1 [10000 / 60000]\n",
            "Train epoch: 1 [12500 / 60000]\n",
            "Train epoch: 1 [15000 / 60000]\n",
            "Train epoch: 1 [17500 / 60000]\n",
            "Train epoch: 1 [20000 / 60000]\n",
            "Train epoch: 1 [22500 / 60000]\n",
            "Train epoch: 1 [25000 / 60000]\n",
            "Train epoch: 1 [27500 / 60000]\n",
            "Train epoch: 1 [30000 / 60000]\n",
            "Train epoch: 1 [32500 / 60000]\n",
            "Train epoch: 1 [35000 / 60000]\n",
            "Train epoch: 1 [37500 / 60000]\n",
            "Train epoch: 1 [40000 / 60000]\n",
            "Train epoch: 1 [42500 / 60000]\n",
            "Train epoch: 1 [45000 / 60000]\n",
            "Train epoch: 1 [47500 / 60000]\n",
            "Train epoch: 1 [50000 / 60000]\n",
            "Train epoch: 1 [52500 / 60000]\n",
            "Train epoch: 1 [55000 / 60000]\n",
            "Train epoch: 1 [57500 / 60000]\n",
            "\n",
            "Test set: Average loss: 0.1631, Accuracy: 9486 / 10000, 95%\n",
            "\n",
            "Train epoch: 2 [0 / 60000]\n",
            "Train epoch: 2 [2500 / 60000]\n",
            "Train epoch: 2 [5000 / 60000]\n",
            "Train epoch: 2 [7500 / 60000]\n",
            "Train epoch: 2 [10000 / 60000]\n",
            "Train epoch: 2 [12500 / 60000]\n",
            "Train epoch: 2 [15000 / 60000]\n",
            "Train epoch: 2 [17500 / 60000]\n",
            "Train epoch: 2 [20000 / 60000]\n",
            "Train epoch: 2 [22500 / 60000]\n",
            "Train epoch: 2 [25000 / 60000]\n",
            "Train epoch: 2 [27500 / 60000]\n",
            "Train epoch: 2 [30000 / 60000]\n",
            "Train epoch: 2 [32500 / 60000]\n",
            "Train epoch: 2 [35000 / 60000]\n",
            "Train epoch: 2 [37500 / 60000]\n",
            "Train epoch: 2 [40000 / 60000]\n",
            "Train epoch: 2 [42500 / 60000]\n",
            "Train epoch: 2 [45000 / 60000]\n",
            "Train epoch: 2 [47500 / 60000]\n",
            "Train epoch: 2 [50000 / 60000]\n",
            "Train epoch: 2 [52500 / 60000]\n",
            "Train epoch: 2 [55000 / 60000]\n",
            "Train epoch: 2 [57500 / 60000]\n",
            "\n",
            "Test set: Average loss: 0.1063, Accuracy: 9656 / 10000, 97%\n",
            "\n",
            "Train epoch: 3 [0 / 60000]\n",
            "Train epoch: 3 [2500 / 60000]\n",
            "Train epoch: 3 [5000 / 60000]\n",
            "Train epoch: 3 [7500 / 60000]\n",
            "Train epoch: 3 [10000 / 60000]\n",
            "Train epoch: 3 [12500 / 60000]\n",
            "Train epoch: 3 [15000 / 60000]\n",
            "Train epoch: 3 [17500 / 60000]\n",
            "Train epoch: 3 [20000 / 60000]\n",
            "Train epoch: 3 [22500 / 60000]\n",
            "Train epoch: 3 [25000 / 60000]\n",
            "Train epoch: 3 [27500 / 60000]\n",
            "Train epoch: 3 [30000 / 60000]\n",
            "Train epoch: 3 [32500 / 60000]\n",
            "Train epoch: 3 [35000 / 60000]\n",
            "Train epoch: 3 [37500 / 60000]\n",
            "Train epoch: 3 [40000 / 60000]\n",
            "Train epoch: 3 [42500 / 60000]\n",
            "Train epoch: 3 [45000 / 60000]\n",
            "Train epoch: 3 [47500 / 60000]\n",
            "Train epoch: 3 [50000 / 60000]\n",
            "Train epoch: 3 [52500 / 60000]\n",
            "Train epoch: 3 [55000 / 60000]\n",
            "Train epoch: 3 [57500 / 60000]\n",
            "\n",
            "Test set: Average loss: 0.0875, Accuracy: 9712 / 10000, 97%\n",
            "\n",
            "Train epoch: 4 [0 / 60000]\n",
            "Train epoch: 4 [2500 / 60000]\n",
            "Train epoch: 4 [5000 / 60000]\n",
            "Train epoch: 4 [7500 / 60000]\n",
            "Train epoch: 4 [10000 / 60000]\n",
            "Train epoch: 4 [12500 / 60000]\n",
            "Train epoch: 4 [15000 / 60000]\n",
            "Train epoch: 4 [17500 / 60000]\n",
            "Train epoch: 4 [20000 / 60000]\n",
            "Train epoch: 4 [22500 / 60000]\n",
            "Train epoch: 4 [25000 / 60000]\n",
            "Train epoch: 4 [27500 / 60000]\n",
            "Train epoch: 4 [30000 / 60000]\n",
            "Train epoch: 4 [32500 / 60000]\n",
            "Train epoch: 4 [35000 / 60000]\n",
            "Train epoch: 4 [37500 / 60000]\n",
            "Train epoch: 4 [40000 / 60000]\n",
            "Train epoch: 4 [42500 / 60000]\n",
            "Train epoch: 4 [45000 / 60000]\n",
            "Train epoch: 4 [47500 / 60000]\n",
            "Train epoch: 4 [50000 / 60000]\n",
            "Train epoch: 4 [52500 / 60000]\n",
            "Train epoch: 4 [55000 / 60000]\n",
            "Train epoch: 4 [57500 / 60000]\n",
            "\n",
            "Test set: Average loss: 0.0792, Accuracy: 9734 / 10000, 97%\n",
            "\n",
            "Train epoch: 5 [0 / 60000]\n",
            "Train epoch: 5 [2500 / 60000]\n",
            "Train epoch: 5 [5000 / 60000]\n",
            "Train epoch: 5 [7500 / 60000]\n",
            "Train epoch: 5 [10000 / 60000]\n",
            "Train epoch: 5 [12500 / 60000]\n",
            "Train epoch: 5 [15000 / 60000]\n",
            "Train epoch: 5 [17500 / 60000]\n",
            "Train epoch: 5 [20000 / 60000]\n",
            "Train epoch: 5 [22500 / 60000]\n",
            "Train epoch: 5 [25000 / 60000]\n",
            "Train epoch: 5 [27500 / 60000]\n",
            "Train epoch: 5 [30000 / 60000]\n",
            "Train epoch: 5 [32500 / 60000]\n",
            "Train epoch: 5 [35000 / 60000]\n",
            "Train epoch: 5 [37500 / 60000]\n",
            "Train epoch: 5 [40000 / 60000]\n",
            "Train epoch: 5 [42500 / 60000]\n",
            "Train epoch: 5 [45000 / 60000]\n",
            "Train epoch: 5 [47500 / 60000]\n",
            "Train epoch: 5 [50000 / 60000]\n",
            "Train epoch: 5 [52500 / 60000]\n",
            "Train epoch: 5 [55000 / 60000]\n",
            "Train epoch: 5 [57500 / 60000]\n"
          ]
        }
      ]
    }
  ]
}